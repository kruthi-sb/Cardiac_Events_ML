{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFetcher' object has no attribute 'get_X_val'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m data_fetcher \u001b[39m=\u001b[39m DataFetcher\u001b[39m.\u001b[39mDataFetcher(\u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mUsers\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mkruth\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mOneDrive\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mDesktop\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mCardiac_Events_ML\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mpreprocessing\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mfinal_dataset.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m X_train \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mget_X_train()\n\u001b[1;32m----> 5\u001b[0m X_val \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39;49mget_X_val()\n\u001b[0;32m      6\u001b[0m X_test \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mget_X_test()\n\u001b[0;32m      7\u001b[0m y_train \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mget_y_train()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFetcher' object has no attribute 'get_X_val'"
     ]
    }
   ],
   "source": [
    "#import DataFetcher class from file heart_data.py\n",
    "import DataFetcher #importing the file DataFetcher.py from the folder data\n",
    "data_fetcher = DataFetcher.DataFetcher(\"C:\\\\Users\\\\kruth\\\\OneDrive\\\\Desktop\\\\Cardiac_Events_ML\\\\preprocessing\\\\final_dataset.csv\")\n",
    "X_train = data_fetcher.get_X_train()\n",
    "X_val = data_fetcher.get_X_val()\n",
    "X_test = data_fetcher.get_X_test()\n",
    "y_train = data_fetcher.get_y_train()\n",
    "y_val = data_fetcher.get_y_val()\n",
    "y_test = data_fetcher.get_y_test()\n",
    "features = data_fetcher.get_features()\n",
    "target_names = data_fetcher.get_target_names()\n",
    "X = data_fetcher.get_X()\n",
    "y = data_fetcher.get_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling the feature dataset:\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "# fit the Scaler\n",
    "scaler = StandardScaler() #creating an instance of the StandardScaler class.\n",
    "#Scaling the values such that the mean is 0 and std deviation is 1.\n",
    "scaler.fit(X_train) #fitting the scaler to the training set.\n",
    "X_train = scaler.transform(X_train) #transforming the training set. \n",
    "X_val = scaler.trandform(X_val) #transforming the validation set.\n",
    "X_test = scaler.transform(X_test) #transforming the testing set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the LogisticRegression class from sklearn.linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# train the model\n",
    "lr_1 = LogisticRegression()\n",
    "\n",
    "#fitting the model to the training set.\n",
    "lr_1.fit(X_train, y_train) \n",
    "\n",
    "# make predictions on the test dataset\n",
    "y_pred_1 = lr_1.predict(X_test)\n",
    "\n",
    "#---------------------------------Evaluating the model---------------------------------\n",
    "\n",
    "# Evaluate the model performance\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print(\"\\n----------Initial Logistic Regression Metrics----------\\n\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_1))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_1))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_1))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_1))\n",
    "#meaning of accuracy score: 0.9 means 90% of the predictions are correct\n",
    "\n",
    "#classification_report\n",
    "print(\"\\n----------Initial Classification Report----------\\n\")\n",
    "from sklearn.metrics import classification_report\n",
    "cr = classification_report(y_test, y_pred_1)\n",
    "print(cr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform grid search to find the best parameters for the model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#creating a dictionary of parameters to be used in the grid search:\n",
    "param_grid = {'C': [0.0001,0.001, 0.01], 'penalty': ['l1', 'l2'], 'solver': ['liblinear','saga'], 'max_iter': [100],'random_state': [42]}\n",
    "#creating an instance of the GridSearchCV class:\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv = 10, scoring='f1')\n",
    "\n",
    "#fitting the grid search to the training set:\n",
    "lr_tuned = grid.fit(X_val, y_val)\n",
    "#finding the best estimator:\n",
    "print(\"\\nbest estimator: \", lr_tuned.best_estimator_)\n",
    "#finding the best score:\n",
    "print(\"best score:  \", lr_tuned.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing of Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the new model with hyperparameters tuned by grid search:\n",
    "#make predictions on the test dataset\n",
    "y_pred = lr_tuned.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print(\"\\n----------Logistic Regression Metrics----------\\n\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "\n",
    "#classification_report\n",
    "print(\"\\n----------Classification Report----------\\n\")\n",
    "from sklearn.metrics import classification_report\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print(cr)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "#visualize the confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\") #fmt=\"d\" means the values in the matrix are integers  \n",
    "#annot=True means the values in the matrix are displayed\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation for 10 folds\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(lr_tuned, X_train, y_train, cv=10, scoring=\"f1\") #scoring can be accuracy, precision, recall, f1, roc_auc, etc\n",
    "#get the scores for each fold\n",
    "for i in range(10):\n",
    "    print(\"Fold\", i, \":\", scores[i])\n",
    "    \n",
    "#get the mean of the scores\n",
    "print(\"\\nMean:\", scores.mean())\n",
    "#get the standard deviation of the scores\n",
    "print(\"Standard Deviation:\", scores.std()) #the lower the standard deviation, the better the model\n",
    "#standard deviation is calculated for the scores of each fold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0058379c129e4629443e4d8323bec08f6392318539cc5f99608b31d35e1b8ca4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
